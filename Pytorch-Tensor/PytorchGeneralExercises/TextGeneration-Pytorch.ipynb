{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'tqdm' is a Python library that allows you to output a smart progress bar by wrapping any iterable. Also, shows the estimated time remaining for the iterable.\n",
    "For import, -conda activate pytorchenv\n",
    "            -conda install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'BeautifulSoup' is a Python library for pulling data out of HTML and XML files. That makes it easy to scrape information from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f071a1a36d5e47c28cc4db9537e763e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=154.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"http://shakespeare.mit.edu/Poetry/\"\n",
    "base_dataset_dir = 'shakespeare_sonnets'\n",
    "\n",
    "## Get page with sonnet list\n",
    "res = requests.get(urljoin(base_url, \"sonnets.html\"))\n",
    "assert res.status_code == 200\n",
    "\n",
    "## Get all sonnet links\n",
    "soup = BeautifulSoup(res.text)\n",
    "all_links = [link.get('href') for link in soup.find_all('a')]\n",
    "all_links = [link for link in all_links if link.startswith('sonnet')]\n",
    "\n",
    "## Download each sonnet\n",
    "for link in tqdm(all_links):\n",
    "  # Get web page with the sonnet\n",
    "  res = requests.get(urljoin(base_url, link))\n",
    "  assert res.status_code == 200\n",
    "  # Convert to proper text\n",
    "  soup = BeautifulSoup(res.text)\n",
    "  sonnet_text = soup.find('blockquote').get_text()\n",
    "  # Save file\n",
    "  sonnet_file = Path(base_dataset_dir) / link.replace('html', 'txt')\n",
    "  sonnet_file.parent.mkdir(exist_ok=True, parents=True) # Create parent dir, if required\n",
    "  with open(sonnet_file, 'w') as f:\n",
    "    f.write(sonnet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing:\n",
    "Read from file -> Random Crop -> Character-level encoding\n",
    "->\n",
    "            One-hot-encoding -> To tensor -> Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SONNET AT INDEX 1\n",
      "O, how much more doth beauty beauteous seem\n",
      "By that sweet ornament which truth doth give!\n",
      "The rose looks fair, but fairer we it deem\n",
      "For that sweet odour which doth in it live.\n",
      "The canker-blooms have full as deep a dye\n",
      "As the perfumed tincture of the roses,\n",
      "Hang on such thorns and play as wantonly\n",
      "When summer's breath their masked buds discloses:\n",
      "But, for their virtue only is their show,\n",
      "They live unwoo'd and unrespected fade,\n",
      "Die to themselves. Sweet roses do not so;\n",
      "Of their sweet deaths are sweetest odours made:\n",
      "  And so of you, beauteous and lovely youth,\n",
      "  When that shall fade, my verse distills your truth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir , transform = None):\n",
    "        # Convert dataset_dir to a Path object\n",
    "        dataset_dir = Path(dataset_dir)\n",
    "        \n",
    "        # Load sonnet from each text file in dataset_dir\n",
    "        self.sonnet_list = []\n",
    "        for sonnet_file in dataset_dir.iterdir():\n",
    "            with open(sonnet_file,'r') as f:\n",
    "                sonnet_text = f.read()\n",
    "            self.sonnet_list.append(sonnet_text)\n",
    "                \n",
    "        # Save the transformation\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sonnet_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Get sonnet text\n",
    "        sample = self.sonnet_list[idx]\n",
    "        \n",
    "        # Transform if defined\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    dataset_dir = 'shakespeare_sonnets'\n",
    "    dataset = ShakespeareDataset(dataset_dir)\n",
    "    \n",
    "    \n",
    "    index = 1\n",
    "    print(f'SONNET AT INDEX {index}')\n",
    "    print(dataset[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transormation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a random substring (encoded) of length \"crop_len\" from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROPPED SAMPLE\n",
      "must, each day say o'er the ve\n"
     ]
    }
   ],
   "source": [
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self,crop_len):\n",
    "        self.crop_len = crop_len\n",
    "    \n",
    "    \n",
    "    def __call__(self,sample):\n",
    "        total_chars = len(sample)\n",
    "        if total_chars <= self.crop_len: # do not crop if samplae is shorter than crop_len\n",
    "            return sample\n",
    "        # Randomly choose an index inside a valid range\n",
    "        start_index = np.random.randint(0, total_chars - self.crop_len)\n",
    "        end_index = start_index + self.crop_len\n",
    "    \n",
    "        # Crop the sample\n",
    "        return sample[start_index: end_index]\n",
    "\n",
    "random_crop = RandomCrop(crop_len=30)\n",
    "cropped_sample = random_crop(dataset[0])\n",
    "print(\"CROPPED SAMPLE\")\n",
    "print(cropped_sample)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character-level encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ord' and 'chr' functions simply convert a character to the corresponding ASCII code and return a list of the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 101, 115, 116, 116]\n"
     ]
    }
   ],
   "source": [
    "class EncodeText():\n",
    "    \n",
    "    def __call__(self,text):\n",
    "        encoded_text = [ord(t) for t in text]\n",
    "        return encoded_text\n",
    "    \n",
    "    \n",
    "encode_text = EncodeText()\n",
    "encoded_text = encode_text(\"testt\")\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testt\n"
     ]
    }
   ],
   "source": [
    "class DecodeText():\n",
    "    \n",
    "    def __call__(self,encoded_text):\n",
    "        decoded_text = [chr(et) for et in encoded_text]\n",
    "        decoded_text = functools.reduce(lambda x, y: x+y, decoded_text)\n",
    "        return decoded_text\n",
    "    \n",
    "decode_text = DecodeText()\n",
    "decoded_text = decode_text(encoded_text)\n",
    "print(decoded_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  From a list of encoded characters (list of values from 0 to 255), convert the \n",
    "  ASCII code to the corresponding letter and return a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each encoded character in \"sample\" in a one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "    \n",
    "    def __call__(self,sample):\n",
    "    \n",
    "    # Create one hot matrix\n",
    "        onehot = np.zeros([len(sample), self.alphabet_len])\n",
    "        tot_chars = len(sample)\n",
    "        onehot[np.arange(tot_chars),sample] = 1\n",
    "    \n",
    "        return onehot\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(10)\n",
    "test_sample = [1,2,5,6,4]\n",
    "onehot = one_hot_encoder(test_sample)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    " # Convert one hot encoded text to pytorch tensor\n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self,sample):\n",
    "        return torch.tensor(sample).float()\n",
    "    \n",
    "to_tensor = ToTensor()\n",
    "tensor = to_tensor(onehot)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composed transform\n",
    "##### Define the preprocessing pipeline, random crop-> character encoding -> one-hot encoding -> to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([50, 255])\n"
     ]
    }
   ],
   "source": [
    "crop_len = 50\n",
    "alphabet_len = 255\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    RandomCrop(crop_len),\n",
    "    EncodeText(),\n",
    "    OneHotEncoder(alphabet_len),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Test all the chain\n",
    "transformed_sample = transform(dataset[0])\n",
    "print(transformed_sample)\n",
    "print(transformed_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 50, 255])\n"
     ]
    }
   ],
   "source": [
    "# Redefine the dataset with the composed transformation\n",
    "\n",
    "dataset_dir = 'shakespeare_sonnets'\n",
    "dataset = ShakespeareDataset(dataset_dir, transform=transform)\n",
    "\n",
    "### Define the dataloader to enable batching and shuffling\n",
    "dataloader = DataLoader(dataset, batch_size=52, shuffle=True)\n",
    "\n",
    "# Test dataloader output\n",
    "batch_sample = next(iter(dataloader))\n",
    "print(batch_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "##### with LSTM, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "   \n",
    "    def __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        super().__init__()\n",
    "    \n",
    "        # define recurrent layers\n",
    "        self.rnn = nn.LSTM(input_size=input_size,\n",
    "                        hidden_size = hidden_units,\n",
    "                        num_layers = layers_num,\n",
    "                        dropout = dropout_prob,\n",
    "                        batch_first = True)\n",
    "    \n",
    "        # define output layer\n",
    "        self.out = nn.Linear(hidden_units, input_size)\n",
    "    \n",
    "    \n",
    "    def forward(self,x,state=None):\n",
    "        \n",
    "       # LSTM\n",
    "        x, rnn_state = self.rnn(x,state)\n",
    "    \n",
    "    # Linear layer\n",
    "        x = self.out(x)\n",
    "    \n",
    "    # Remember to return also the RNN state, you will need it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 255\n",
    "hidden_units = 128\n",
    "layers_num = 128\n",
    "dropout_prob = 0.3\n",
    "net = Network(input_size,hidden_units,layers_num,dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-660ba40eb31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Test the network output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Out shape: \\t\\t{out.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hidden state shape: \\t{rnn_state[0].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Test network with one batch\n",
    "batch_sample = next(iter(dataloader))\n",
    "    \n",
    "# Test the network output\n",
    "out, rnn_state = net(batch_sample)\n",
    "print(f\"Out shape: \\t\\t{out.shape}\")\n",
    "print(f\"Hidden state shape: \\t{rnn_state[0].shape}\")\n",
    "print(f\"Cell state shape: \\t{rnn_state[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.RMSprop(net.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acb38f9c26b4b88a3ff47a0faa03fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-40800cef6452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m## we dont need to rnn state at this point, we can ignore the output with \"_\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'training device: {device}')\n",
    "\n",
    "net.to(device) # move network to the proper device\n",
    "net.train() # network in training mode\n",
    "\n",
    "# Iterate through the dataloader for num_epochs\n",
    "num_epochs = 1000\n",
    "for num_epochs in tqdm(range(num_epochs)):\n",
    "    epoch_losses = []\n",
    "    for batch_sample in dataloader:\n",
    "        \n",
    "        # Move samples to proper device\n",
    "        batch_sample = batch_sample.to(device)\n",
    "        \n",
    "        #P repare network input and labels\n",
    "        net_input = batch_sample[:,:-1,:]\n",
    "        labels= batch_sample[:,1:, :]\n",
    "        \n",
    "        # Forward pass\n",
    "        # Clear previous recorded gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        net_out, _ = net(net_input)\n",
    "## we dont need to rnn state at this point, we can ignore the output with \"_\"\n",
    "\n",
    "        #Update network\n",
    "        labels = labels.argmax(dim=-1)\n",
    "        net_out = net_out.permute([0,2,1])\n",
    "        loss = loss_fn(net_out, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save batch loss\n",
    "        epoch_losses.append(loss.data.cpu().numpy())\n",
    "        \n",
    "        # print avg epoch loss\n",
    "        print(f'epocsh {num_epochs + 1} loss: {np.mean(epoch_losses)}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
